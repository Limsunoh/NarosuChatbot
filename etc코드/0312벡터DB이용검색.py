from dotenv import load_dotenv
import os
import pandas as pd
import json
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
import faiss
import numpy as np
from datetime import datetime
from fastapi import FastAPI, HTTPException, Request
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import RedisChatMessageHistory
from langchain_core.runnables import ConfigurableFieldSpec
from typing import Union
from concurrent.futures import ThreadPoolExecutor
from sentence_transformers import SentenceTransformer


executor = ThreadPoolExecutor()

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ+
load_dotenv()
API_KEY = os.getenv('OPENAI_API_KEY')
REDIS_URL = "redis://localhost:6379/0"
VERIFY_TOKEN = os.getenv('VERIFY_TOKEN')
PAGE_ACCESS_TOKEN = os.getenv('PAGE_ACCESS_TOKEN')
MANYCHAT_API_KEY = os.getenv('MANYCHAT_API_KEY')

print(f"ğŸ” ë¡œë“œëœ VERIFY_TOKEN: {VERIFY_TOKEN}")
print(f"ğŸ” ë¡œë“œëœ PAGE_ACCESS_TOKEN: {PAGE_ACCESS_TOKEN}")

# âœ… BERT ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (FAISSì—ì„œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±)
BERT_MODEL_NAME = "sentence-transformers/paraphrase-MiniLM-L6-v2"
embedding_model = SentenceTransformer(BERT_MODEL_NAME)

# âœ… FAISS ì¸ë±ìŠ¤ íŒŒì¼ & ì—¬ëŸ¬ ê°œì˜ ì—‘ì…€ ë°ì´í„° íŒŒì¼
FAISS_INDEX_PATH = "faiss_index_03M.faiss"
excel_files = [
    "db/file1.xlsx",
    "db/file2.xlsx",
    "db/file3.xlsx",
    "db/file4.xlsx"
]

# âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ
if os.path.exists(FAISS_INDEX_PATH):
    index = faiss.read_index(FAISS_INDEX_PATH)
    print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({index.ntotal} ê°œì˜ ë²¡í„° í¬í•¨)")
else:
    raise FileNotFoundError(f"âŒ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {FAISS_INDEX_PATH}")

# âœ… ì—‘ì…€ ë°ì´í„° ë¡œë“œ ë° ê° íŒŒì¼ì˜ ë°ì´í„° ë²”ìœ„ ì €ì¥
excel_dataframes = []
data_ranges = []  # ê° ì—‘ì…€ íŒŒì¼ì˜ ë°ì´í„° ë²”ìœ„ë¥¼ ì €ì¥

start_idx = 0
for excel_file in excel_files:
    if os.path.exists(excel_file):
        df = pd.read_excel(excel_file)
        excel_dataframes.append(df)
        data_ranges.append((start_idx, start_idx + len(df), excel_file))  # (ì‹œì‘ ì¸ë±ìŠ¤, ë ì¸ë±ìŠ¤, íŒŒì¼ëª…)
        start_idx += len(df)
        print(f"âœ… ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(df)} ê°œì˜ ìƒí’ˆ ì •ë³´) - {excel_file}")
    else:
        print(f"âš ï¸ ê²½ê³ : ì—‘ì…€ íŒŒì¼ ì—†ìŒ - {excel_file}")

# âœ… FAISS ê²€ìƒ‰ í•¨ìˆ˜ (ê²€ìƒ‰ í›„ ì–´ëŠ ì—‘ì…€ íŒŒì¼ì˜ ë°ì´í„°ì¸ì§€ ì°¾ìŒ)
def search_similar_documents(query, top_k=5):
    """
    (1) ê²€ìƒ‰ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ í›„ FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
    (2) ê²€ìƒ‰ëœ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ëŠ ì—‘ì…€ íŒŒì¼ì˜ ë°ì´í„°ì¸ì§€ ì°¾ìŒ
    """
    query_embedding = embedding_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)

    # âœ… FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
    distances, indices = index.search(query_embedding, top_k)

    results = []
    
    for dist, idx in zip(distances[0], indices[0]):
        # âœ… ê²€ìƒ‰ëœ ì¸ë±ìŠ¤(idx)ê°€ ì–´ëŠ ì—‘ì…€ íŒŒì¼ì˜ ë°ì´í„°ì¸ì§€ í™•ì¸
        for start_idx, end_idx, file_name in data_ranges:
            if start_idx <= idx < end_idx:
                df_index = idx - start_idx  # âœ… í•´ë‹¹ ì—‘ì…€ íŒŒì¼ì—ì„œì˜ ì¸ë±ìŠ¤ ë³€í™˜
                df = pd.read_excel(file_name)  # âœ… í•´ë‹¹ ì—‘ì…€ íŒŒì¼ ë¡œë“œ
                product_info = df.iloc[df_index].to_dict()  # âœ… ìƒí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                
                results.append({
                    "source": file_name,
                    "index": df_index,
                    "distance": dist,
                    "product_info": product_info
                })
                break

    return sorted(results, key=lambda x: x["distance"])  # âœ… ê±°ë¦¬ ìˆœ ì •ë ¬

# âœ… CMDì—ì„œ ê²€ìƒ‰ ì‹¤í–‰
if __name__ == "__main__":
    while True:
        query = input("\nğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
        if query.lower() == "exit":
            print("ğŸšª í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
            break

        results = search_similar_documents(query)

        if results:
            print("\nğŸ“Œ ê²€ìƒ‰ ê²°ê³¼:")
            for i, result in enumerate(results):
                print(f"{i+1}. [íŒŒì¼: {result['source']}, ì¸ë±ìŠ¤: {result['index']}, ê±°ë¦¬: {result['distance']:.4f}]")
                print("    ğŸ“„ ì œí’ˆ ì •ë³´:")

                # âœ… ì£¼ìš” ì œí’ˆ ì •ë³´ ì¶œë ¥
                product_info = result["product_info"]
                print(f"    ğŸ·ï¸ ì œëª©: {product_info.get('ìƒí’ˆì½”ë“œ', 'N/A')}")
                print(f"    ğŸ’° ê°€ê²©: {product_info.get('ì˜¤ë„ˆí´ëœíŒë§¤ê°€', 'N/A')} ì›")
                print(f"    ğŸ“¦ ì›ì‚°ì§€: {product_info.get('ì›ì‚°ì§€', 'N/A')}")
                print(f"    ğŸ“ ì„¤ëª…: {product_info.get('ì›ë³¸ìƒí’ˆëª…', 'N/A')}")
                print(f"    ğŸ–¼ï¸ ì´ë¯¸ì§€ ë§í¬: {product_info.get('ì´ë¯¸ì§€ì¤‘', 'N/A')}")

                print("-" * 80)
        else:
            print("âŒ ê²€ìƒ‰ ì‹¤íŒ¨")